\documentclass{article}
\usepackage[sorting=none]{biblatex}
\usepackage{listings}
\usepackage{amsmath}
\lstset{
    frame=tb,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=4
}
\addbibresource{report.bib}
\title{CS890BR Project}
\date{\today}
\author{Riley Herman 200352833}

\begin{document}
    \pagenumbering{gobble}
    \maketitle
    \begin{abstract}
        this is the abstract. you know
    \end{abstract}
    \newpage
    \pagenumbering{arabic}
    \section{Introduction}
    In the early days of the stock market, William Stanley Jevons developed a widely used theory on how to predict changes in the market.
    This is the man that wrote such influential books as The Theory of Political Economy, expounding the marginal utility theory of value that 
    is fundamental to understanding modern economics. A first car has enormous value in comparison to a fifteenth car. Unfortunately for Jevons, 
    his market prediction theory was not so revolutionary. William Peter Hamilton recounts, "[Jevons] propounded the theory of a connection between 
    commercial panics and spots on the sun. \ldots I said that while Wall Street in its heart believed in a cycle fo panic and prosperity, 
    it did not care if there were enough spots of the sun to make a straight flush" \cite{Hamilton}. As much as Jevons' sun spot prediction method
    sounds absurd, the inherent nature of the stock market is to be unpredictable. His theory is every bit as valid as any other when the theory 
    attempts to explain a complex adaptive system. Michael Mauboussin acknowledges that "[m]ost systems, in nature and in business,
    are not in equilibrium but rather in constant flux" \cite{Mauboussin}. \\
    Having said that, and despite the orange tabby cat Orlando's success as a stock picker \cite{King}, there is little disagreement that making
    informed portfolio choices is in the best interest of the investor. Therefore, using historical data to generate a set of optimal portfolios
    is the widely accepted approach. Given the multitude of solving techniques, it is in the investor's best interest to determine the most efficient solver
    to find the most efficient solution. There are five algorithms presented in this paper: Flower Pollination, Non-dominated Sorting Genetic 
    Algorithm 2 (hereby referred to as NSGA-II), Particle Swarm Optimization (PSO), Strength Pareto Evolutionary Algorithm 2 (SPEA-2), and traditional
    Branch and Bound, an exact algorithm the other algorithms are to be compared against \cite{Yang} \cite{KaucicMoradiMirzazadeh} \cite{Guerard}. 
    \section{Background Knowledge}
    In order to understand the recommended portfolios and how they are arrived at, one must first understand the problem that is being solved, how it is 
    modelled, what techniques are available and why they were chosen. 
    \subsection{Portfolio Optimization}
    The problem of portfolio optimization often finds its roots in Harry Markowitz's frequently cited book Portfolio Selection \cite{Markowitz}. Though
    the language may not be the same, what he is proposing is clearly a multi-objective constraint optimization problem. He suggests that portfolios
    should be weighted using risk and return with respect to a budgetary constraint. Though the problem originates in the field of 
    economics, computing has approached the problem as an excellent candidate for a variety of multi-objective constraint optimization algorithms. \\
    The problem is formulated as follows: given a set of stocks and a budget, one must use the risk and reward to find an amount of each stock to
    purchase such that risk is minimal and reward is maximal. Risk and reward are not comparable to each other; the algorithm cannot decide whether
    a conservative, low reward but also low risk portfolio is better than an aggressive, high risk but high reward portfolio. Thus the solution is a 
    set of options, each one best in its own way. There are several pieces of information that need to be calculated: prices, which are often taken 
    directly from source data; reward, which is NEED TO FILL IN HOW REWARD WORKS; and risk, which was once upon a time calculated as variance
    but is now often calculated as value-at-risk. Value at risk is a type of downside risk; that is, it attempts to measure risk by estimating the
    maximum amount lost on the stock \cite{HongHuZhang} \cite{Cid}. There are a variety of other objectives that could be optimized towards as well: 
    liquiditiy, tax efficiency, etc. However, since the premise of these quantitative optimizations are the same it has been left out of the scope 
    of this project. Another set of objectives that could be optimized towards are qualitative; examples of this may be longevity of the portfolio,
    where a longer term portfolio may be able to accept more risk than a shorter term portfolio \cite{Xiongetal}, or social responsibility of the 
    companies being traded. Once the problem is formulated in terms of variables (the stocks and how much to buy of each one), constraints (the 
    total amount invested must be below the budget), and objective functions (risk and reward), the problem is ready to be modelled as a constraint
    optimization problem.
    \subsection{Constraint Optimization Techniques}
    A constraint satisfaction problem is defined as a problem with variables, each with a domain and constrained by constraints. The portfolio
    problem in this form is not particularily useful, because any selection of any amount of the stocks such that the total amount spent does 
    not exceed the budget satifies the constraint. Thus the advice may be to not buy anything at all! A constraint optimization problem is a 
    constraint satisfaction problem with the addition of one or more objective functions. An objective function is a function that is minimized
    or maximized. For example, Dave is packing his backpack for a trip. He cannot fill it past the point at which he can no longer 
    carry it, but filling it with bricks will not help him on his camping trip. He must fill it with the items that offer the most utility
    without exceeding the weight constraint. The utility, in this case, must be maximized with respoect given to the constraint. 
    \subsubsection{Branch and Bound}
    Branch and bound is one example of a solving technique for constraint optimization problems. This is an exact method, and will give the 
    best answer of any algorithm presented here. However, the time and space complexity of this algorithm is of major concern. That is what
    gave rise to the inexact methods discussed below. Branch and bound can be implemented by the following pseudo-code:
    \begin{lstlisting}
        define BranchAndBound(node, collection, solutions):
            if node is a leaf node and node is consistent then:
                solutions = solutions + node
                if node->bound > lower_bound then:
                    lower_bound := node->bound
                return solutions
            end if
            for variable in node->variables do:
                if variable has value then:
                    continue
                end if
                for value in variable->domain do:
                    set value for variable
                    if node is consistent and lower_bound < node->bound then:
                        collection = collection + node
            new_node = collection->top
            collection = collection - new_node
            return BranchAndBound(new_node, collection, solutions)
    \end{lstlisting}
    A \(node\) is a representation of the problem. It has several properties: it keeps track of the variables, and each variable keeps track of its 
    domain and current value. It becomes a leaf node when all of its variables have values. When all constraints are satisfied, it is consistent.
    A node also knows its bound; this is a value that corresponds to its objective functions and allows the algorithm to 
    trim any nodes which would inevitably result in a worse solution than the solutions that have already been obtained. To extend on the example 
    of Dave's camping trip, if Dave has a few items in his backpack that are less than ideal, Dave may realize that even if he adds the most 
    utilitous items left, he won't acheive as good of a backpack as he has already come up with. Therefore there's no point in continuing to add 
    items to this backpack. The collection object can be done in a variety of ways, similar to most searching algorithms. If the collection is 
    a stack, then the algorithm will perform a depth first style of search. It's worth noting that this isn't a true depth first search as it has the
    added bounding logic. A queue results in a breadth first style fo search, and a priority queue (using a heuritic value) will result in a
    best first style of search. \cite{LandDoig} \cite{Apt} \cite{Dechter}
    \subsubsection{Genetic Algorithms}
    Genetic algorithms are a subset of evolutionary algorithms that are unified through the use of some general nature-inspired functions. These
    functions include mutation, where a gene changes itself; crossover, where two genes swap part of their sequence; and selection, where the 
    best genes are selected for the next roud of crossover and mutation. Many of these algorithms are explained through the metaphors that they 
    borrow their names from; for example, flower pollination. Flower pollination is, intuitively, based on the pollination of flowering plants. 
    There are four rules that govern this algorithm:
    \begin{itemize}
        \item Biotic, or global pollination
        \item Abiotic, or local pollination, 
        \item Flower constancy
        \item Switch probability governing the decision to locally or globally pollinate
    \end{itemize}
    Global pollination is similar to the generic crossover genetic function. The candidate that is chosen by the pollinator is chosen using 
    a L\'evy flight. A L\'evy flight is a type of random walk where the randomly chosen step follows a L\'evy distribution, named for 
    mathematician Paul L\'evy. It is a left-skewed distribution so the steps will generally not be particularly aggressive. This makes the 
    chances of a valid solution becoming invalid more manageable. 
    Local pollination is similar to the genetic function of mutation. The pollinator steps with generally the same velocity with which it 
    took its previous step, using a uniformly distributed random factor.
    Flower constancy is a biological phenomenon where a pollinator will often choose the same flower species over and over again, despite 
    other (possibly better) alternatives being present. This is preserved in the algorithm by reproductive ratio, which is proportional
    to the degree of similar between the pollinator and its pollination candidate. 
    The fourth rule of flower pollination is the switch probability. On any given pollination event, the choice between biotic and abiotic
    pollination is determined by random chance. The algorithm, using all of these parts, can be expressed by the following pseudocode:
    \begin{lstlisting}
        define FlowerPollination():
            flowers := initial random population of size n
            g* := find best solutions in flowers
            generation := 0
            while generation < MAX_GENERATIONS do:
                for flower in flowers do:
                    if rand > SWITCH_PROBABILITY then:
                        l := draw from Levy distribution
                        flower->value := flower->value + l(g*->value - flower->value)
                    else then:
                        e := random uniform value in [0, 1]
                        flower_a, flower_b := two random flowers from the set of solutions
                        flower->value := flower->value + e(flower_a->value - flower_b->value)
                    end if
                    g* := find best solutions in flowers
                end for
            end while
    \end{lstlisting}\cite{Nabil} \cite{AroraAnand}
    Some of the constants used in this algorithm include the maximum number of generations and the switch probability, which are generally configurable and vary 
    from problem to problem. \\
    Proposed in 1995, particle swarm optimization (PSO) can be considered the grandfather of some of the other algorithms discussed here. PSO is an algorithm in 
    which particles swarm towards an optimal goal. A particle has a few components:
    \begin{itemize}
        \item values for each variable
        \item a velocity for each variable
        \item the personal best solution that has been found
        \item the global best solution that has been found by the swarm
    \end{itemize}
    The PSO algorithm can be expressed by the following pseudocode:
    \begin{lstlisting}
        define ParticleSwarmOptimization():
            particles := initial random population of size n, each with random velocities
            global_best := particles
            while stopping condition not met do:
                for particle in particles do:
                    if particle->position > particle->best then:
                        particle->best := particle->position
                    end if 
                    for best in global_best do:
                        if particle->best > best then:
                            global_best := global_best + particle->best
                            global_best := global_best - best
                        end if
                    particle->position := particle->position + particle->velocity
                    particle->update_velocity()
                end for
            end while
            return global_best
    \end{lstlisting}
    Note that \(particle \rightarrow update\_velocity()\) is left as a function in this pseudocode. This function has three components:
    \begin{itemize}
        \item intertia, or \(w \times particle \rightarrow velocity\)
        \item the cognitive component, which is a \(C \times rand_1 \times (particle \rightarrow best - particle \rightarrow value)\)
        \item the social component, which is a \(S \times rand_2 \times (global\_best - particle \rightarrow value)\)
    \end{itemize}
    where \(w\), \(C\), and \(S\) are all predefined constants. It then returns the sum of the components. \cite{EberhartKennedy} \cite{KhanesarTavakoliTeshnehlabShoorehdeli} \\
    SPEA-2 (as well as NSGA-II), while still using genetic components, are not named for a natural metaphor. It proceeds as follows:
    \begin{lstlisting}
        define SPEA-2():
            population := initial random population of size n
            archive := empty set
            generation := 0
            while generation < MAX_GENERATIONS do:
                dominated := empty set
                for individual in population + archive do:
                    for other in population + archive do:
                        if other dominates individual then:
                            dominated := dominated + individual
                        else if individual dominates other then:
                            dominated := dominated + other
                        end if
                    end for
                end for
                non_dominated := (population + archive) - dominated
                archive := non_dominated
                if |archive| < n then:
                    dominated := sort_by_domination(dominated)
                    archive := archive + dominated
                end if
                if |archive| > n then:
                    archive := truncate(archive)
                end if
                population := binary_tournament_selection()
            return archive
    \end{lstlisting}
    Several functions have been left as functions in this pseudocode: \(dominates\), \(sort\_by\_domination\), \(truncate\), and \(binary\_tournament\_selection\). 
    \(dominates\) is a common function in multi-objective solvers; it is used to compare solutions without having to consolidate the objective functions
    into a single, comparable value. Each objective value is compared to the respective objective value of another solution. If all are equal or greater and 
    at least one is strictly greater, then the solution dominates the other solution. Naturally, this has several useful properties such as the transitive 
    property. Sorting by domination, as in \(sort_by_domination\), is to put all solutions in an order such that no solution ahead of a given solution dominates 
    that solution. The \(truncate\) operation simply shortens the list to the proper \(n\) size required for the next iteration. Since the list is sorted (at 
    least at the rear of the list), this means that the solutions that are discarded are the most dominated and will not reveal the best results. Finally, 
    \(binary\_tournament\_selection\) uses a binary tournament selection method to select the next generation of solutions. First, the prospective individuals
    are sorted by domination. Then, according to their rank in the order, they are assigned probabilities for selection. The non-dominated have the best 
    probability of being chosen, while the most dominated have the worst probability of being chosen. Then, the next population is randomly chosen from this 
    pool of individuals. \cite{ZitzlerLaumannsThiele} \cite{KaucicMoradiMirzazadeh} \\
    In SPEA-2, sorting is heavily relied upon. Non-dominated Sorting Genetic Algorithm II relies even more heavily on sorting a population and picking the best from 
    the front of the list. There are two functions that NSGA-II uses: \(fast\_non\_dominated\_sort\) and \(crowding\_distance\_assignment\). \(fast\_non\_dominated\_sort\)
    is as follows:
    \begin{lstlisting}
        define fast_non_dominated_sort(population):
            front = set with a single empty set as the first item
            for individual_p in population do:
                individual_p->dominates := empty set
                individual_p->is_dominated_count := 0
                for individual_q in population do:
                    if individual_p dominates individual_q then:
                        individual_p->dominates := individual_p->dominates + individual_q
                    else if individual_q dominates individual_p then:
                        individual_p->is_dominated_count = individual_p->is_dominated_count + 1
                    end if
                end for
                if individual_p->is_dominated_count = 0 then:
                    individual_p->rank := 0
                    front[start] := front[start] + individual_p
                end if
            end for
            i := 0
            while front[i] is not empty do:
                next_front := empty set
                for individual_p in front[i] do:
                    for each individual_q in individual_p->dominates do:
                        individual_q->is_dominated_count := individual_q->is_dominated_count - 1
                        if individual_q->is_dominated_count = 0 then:
                            individual_q->rank := i
                            next_front := next_front + individual_q
                        end if
                    end for
                end for
                i := i + 1
                front[i] = next_front
            end while
    \end{lstlisting}
    \(crowding\_distance\_assignment\) is as follows:
    \begin{lstlisting}
        define crowding_distance_assignment(front):
            for individual in front do:
                individual->crowding_distance := 0
            end for
            for objective in individual->objectives do:
                front := sort_by_objective_value(front, objective)
                front[start]->crowding_distance, front[end]->crowding_distance := infinity
                for start + 1 < i < end - 1 do:
                    front[i]->crowding_distance = calculate_crowding_distance(front[i]->crowding_distance, front[i + 1]->objectives[objective], front[i - 1]->objectives[objective])
                end for
            end for
    \end{lstlisting}
    where \(calculate\_crowding\_distance(a, b, c)\) is the following formula:
    \begin{equation*}
        a = a + \dfrac{(b - c)}{f_max - f_min}
    \end{equation*}
    and \(f_max\), \(f_min\) are the maximum and minimum possible values for the objective in question. 
    Finally, the full NSGA-II is as follows:
    \begin{lstlisting}
        define NSGA-II():
            generation := 0
            P := initial random population of size n
            while generation < MAX_GENERATIONS do:
                F = fast_non_dominated_sort(P + Q)
                next_population := empty set
                i := 0
                while |next_population| + |F[i]| < n do:
                    crowding_distance_assignment(F[i])
                    next_population := next_population + F[i]
                    i := i + 1
                end while
                next_population := next_population + F[i][begin - (n-|next_population|)]
                Q := binary_tournament_selection()
                generation := generation + 1
            end while
            return F[0]
    \end{lstlisting}
    This algorithm uses a similar \(binary\_tournament\_selection\) as SPEA-2.
    \cite{DebPratapAgarwalMeyarivan} \cite{PonsichJaimesCoello}
    \section{Implementation}
    \subsection{Data}
    I got the data from Finnhub
    \subsection{Code}
    I wrote it in python 3
    \section{Conclusion}
    This is the conclusion where we recap all the fun we had this past week.
    \newpage
    \printbibliography
\end{document}