\documentclass{article}
\usepackage[sorting=none]{biblatex}
\usepackage{listings}
\usepackage{amsmath}
\lstset{
    frame=tb,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=4
}
\addbibresource{report.bib}
\title{CS890BR Project}
\date{\today}
\author{Riley Herman 200352833}

\begin{document}
    \pagenumbering{gobble}
    \maketitle
    \begin{abstract}
        Portfolio optimization is an excellent candidate for multi-objective constraint optimization techniques. Branch and bound, PSO, flower 
        pollination, NSGA-II, and SPEA-2 were implemented and compared to solve the portfolio optimization problem. Branch and bound is an 
        exact method of solving constraint optimization problems that mirrors common searching techniques. The other techniques explored are 
        all from a family of algorithms inspired by genetics, called genetic algorithms. PSO uses many initial particles that
        swarm around the best solutions over several generations. Flower pollination uses an initial field of flowers and has an intelligent
        pollinator that will either locally or globally pollinate flowers to find the best bouquet. NSGA-II sorts individual solutions into 
        fronts such that the first front is anything that is not dominated, the second front is dominated by one solution, and so on and so forth.
        Over several iterations of this, the first front becomes the best solutions. SPEA-2 samples repeatedly from a tournament pool made up of some 
        random new solutions and the non-dominated solutions from the previous round. Each algorithm is given in pseudocode and detailed description.
        The implementations are compared; while branch and bound was sampled as the fastest algorithm despite it's larger computational and spatial 
        complexity, there are external factors that may have influenced this result. Of the genetic algorithms, PSO was fastest and was the least 
        restrictive in the solutions generated. Flower pollination had the fewest solutions generated but all were of high quality. NSGA-II and SPEA-2
        had similar solutions except in terms of crowding, where NSGA-II had a more spread out front of solutions and SPEA-2 (as well as all of the 
        other genetic algorithms) had some clustering. 
    \end{abstract}
    \newpage
    \pagenumbering{arabic}
    \section{Introduction}
    In the early days of the stock market, William Stanley Jevons developed a widely used theory on how to predict changes in the market.
    This is the man that wrote such influential books as The Theory of Political Economy, expounding the marginal utility theory of value that 
    is fundamental to understanding modern economics \cite{Jevons}. A first car has enormously more value than a fifteenth car. Unfortunately for Jevons, 
    his market prediction theory was not so revolutionary. William Peter Hamilton recounts, ``[Jevons] propounded the theory of a connection between 
    commercial panics and spots on the sun. \ldots I said that while Wall Street in its heart believed in a cycle of panic and prosperity, 
    it did not care if there were enough spots of the sun to make a straight flush'' \cite{Hamilton}. As much as Jevons' sun spot prediction method
    sounds absurd, the inherent nature of the stock market is to be unpredictable. His theory is every bit as valid as any other when the theory 
    attempts to explain a complex adaptive system. Michael Mauboussin acknowledges that ``[m]ost systems, in nature and in business,
    are not in equilibrium but rather in constant flux'' \cite{Mauboussin}. \\
    Having said that, and despite the orange tabby cat Orlando's success as a stock picker \cite{King}, there is little disagreement that making
    informed portfolio choices is in the best interest of the investor. Therefore, using historical data to generate a set of optimal portfolios
    is the widely accepted approach. Given the multitude of solving techniques, it is in the investor's best interest to determine the most efficient solver
    to find the most efficient solution. There are five algorithms presented here: Flower Pollination, Non-dominated Sorting Genetic 
    Algorithm II (hereby referred to as NSGA-II), Particle Swarm Optimization (PSO), Strength Pareto Evolutionary Algorithm 2 (SPEA-2), and traditional
    Branch and Bound, an exact algorithm the other algorithms are to be compared against \cite{Yang} \cite{KaucicMoradiMirzazadeh} \cite{Guerard}. 
    \section{Background Knowledge}
    In order to understand the recommended portfolios and how they are arrived at, one must first understand the problem that is being solved, how it is 
    modelled, what techniques are available and why they were chosen. 
    \subsection{Portfolio Optimization}
    The problem of portfolio optimization often finds its roots in Harry Markowitz's frequently cited book Portfolio Selection \cite{Markowitz}.
    The language he uses is not the same language a computing problem is generally expressed, but what he is proposing is clearly a multi-objective 
    constraint optimization problem. He suggests that portfolios should be weighted using risk and return with respect to a budgetary constraint.
    The problem is formulated as follows: given a set of stocks and a budget, one must use the risk and reward to find an amount of each stock to
    purchase such that risk is minimal and reward is maximal. Risk and reward are not comparable to each other; the algorithm cannot decide whether
    a conservative, low reward but also low risk portfolio is better than an aggressive, high risk but also high reward portfolio. Thus the solution is a 
    set of options, each one best in its own way. There are several pieces of information that need to be calculated: prices, which are often taken 
    directly from source data; reward, which is taken as the high target value (or the highest value in the time period); and risk, which was once 
    upon a time calculated as variance but is now often calculated as value-at-risk. Value at risk is a type of downside risk; that is, it attempts 
    to measure risk by estimating the maximum amount lost on the stock \cite{HongHuZhang} \cite{Cid}. There are a variety of other objectives that 
    could be optimized towards as well: liquiditiy, tax efficiency, etc. However, since the premise of these quantitative optimizations are the same 
    it has been left out of the scope of this program. Another set of objectives that could be optimized towards are qualitative; examples of this 
    may be longevity of the portfolio, where a longer term portfolio may be able to accept more risk than a shorter term portfolio \cite{Xiongetal}, 
    or social responsibility of the companies being traded. These have also been left out of the scope of this project for the complexities involved. 
    Once the problem is formulated in terms of variables (the stocks and how much to buy of each one), constraints (the total amount invested must be 
    below the budget), and objective functions (risk and reward), the problem is ready to be modelled as a constraint optimization problem.
    \subsection{Constraint Optimization Techniques}
    A constraint satisfaction problem is defined as a problem with variables, each with a domain and constrained by constraints. The portfolio
    problem in this form is not particularily useful, because any selection of any amount of the stocks such that the total amount spent does 
    not exceed the budget satifies the constraint. Thus the advice may be to not buy anything at all! A constraint optimization problem is a 
    constraint satisfaction problem with the addition of one or more objective functions. An objective function is a function that is minimized
    or maximized. For example, Dave is packing his backpack for a trip. He cannot fill it past the point at which he can no longer 
    carry it, but filling it with bricks will not be useful to him on his camping trip. He must fill it with the items that offer the most utility
    without exceeding the weight constraint. The utility, in this case, must be maximized with respect given to the constraint. 
    \subsubsection{Branch and Bound}
    Branch and bound is one example of a solving technique for constraint optimization problems. This is an exact method, and will give the 
    best answer of any algorithm presented here. However, the time and space complexity of this algorithm is of major concern. That is what
    gave rise to the inexact methods discussed below. Branch and bound can be implemented by the following pseudocode:
    \begin{lstlisting}
        define BranchAndBound(node, collection, solutions):
            if node->bound < lower_bound then:
                new_node = collection->top
                collection = collection - new_node
                return ranchAndBound(new_node, collection, solutions)
            if node is a leaf node and node is consistent then:
                solutions = solutions + node
                if node->bound > lower_bound then:
                    lower_bound := node->bound
                return solutions
            end if
            for variable in node->variables do:
                if variable has value then:
                    continue
                end if
                for value in variable->domain do:
                    set value for variable
                    if node is consistent and node->bound > lower_bound then:
                        collection = collection + node
            new_node = collection->top
            collection = collection - new_node
            return BranchAndBound(new_node, collection, solutions)
    \end{lstlisting}
    A \(node\) is a representation of the problem. It has several properties: it keeps track of the variables, and each variable keeps track of its 
    domain and current value. It becomes a leaf node when all of its variables have values. When all constraints are satisfied, it is consistent.
    A node also knows its bound; this is a value that corresponds to its objective functions and allows the algorithm to 
    trim any nodes which would inevitably result in a worse solution than the solutions that have already been obtained. To extend on the example 
    of Dave's camping trip, if Dave has a few items in his backpack that are less than ideal, Dave may realize that even if he adds the most 
    utilitous items left, he won't acheive as good of a backpack as he has already come up with. Therefore there's no point in continuing to add 
    items to this backpack. The collection object can be done in a variety of ways, similar to most searching algorithms. If the collection is 
    a stack, then the algorithm will perform a depth first style of search. It's worth noting that this isn't a true depth first search as it has the
    added bounding logic. A queue results in a breadth first style fo search, and a priority queue (using a heuritic value) will result in a
    best first style of search. \cite{LandDoig} \cite{Apt} \cite{Dechter}
    \subsubsection{Genetic Algorithms}
    Genetic algorithms are a subset of evolutionary algorithms that are unified through the use of some general nature-inspired functions. These
    functions include mutation, where a gene changes itself; crossover, where two genes swap part of their sequence; and selection, where the 
    best genes are selected for the next roud of crossover and mutation. Many of these algorithms are explained through the metaphors that they 
    borrow their names from; for example, flower pollination. Flower pollination is, intuitively, based on the pollination of flowering plants. 
    There are four rules that govern this algorithm:
    \begin{itemize}
        \item Biotic, or global pollination
        \item Abiotic, or local pollination, 
        \item Flower constancy
        \item Switch probability governing the decision to locally or globally pollinate
    \end{itemize}
    Global pollination is similar to the generic crossover genetic function. The candidate that is chosen by the pollinator is chosen using 
    a L\'evy flight. A L\'evy flight is a type of random walk where the randomly chosen step follows a L\'evy distribution, named for 
    mathematician Paul L\'evy. It is a left-skewed distribution so the steps will generally not be particularly aggressive. This makes the 
    chances of a valid solution becoming invalid more manageable. 
    Local pollination is similar to the genetic function of mutation. The pollinator steps with generally the same direction with which it 
    took its previous step, using a uniformly distributed random factor.
    Flower constancy is a biological phenomenon where a pollinator will often choose the same flower species over and over again, despite 
    other (possibly better) alternatives being present. This is preserved in the algorithm by reproductive ratio, which is proportional
    to the degree of similar between the pollinator and its pollination candidate. 
    The fourth rule of flower pollination is the switch probability. On any given pollination event, the choice between biotic and abiotic
    pollination is determined by random chance. The algorithm, using all of these parts, can be expressed by the following pseudocode:
    \begin{lstlisting}
        define FlowerPollination():
            flowers := initial random population of size n
            g* := find best solutions in flowers
            generation := 0
            while generation < MAX_GENERATIONS do:
                for flower in flowers do:
                    if rand > SWITCH_PROBABILITY then:
                        l := draw from Levy distribution
                        flower->value := flower->value + l(g*->value - flower->value)
                    else then:
                        e := random uniform value in [0, 1]
                        flower_a, flower_b := two random flowers from the set of solutions
                        flower->value := flower->value + e(flower_a->value - flower_b->value)
                    end if
                    g* := find best solutions in flowers
                end for
            end while
    \end{lstlisting}\cite{Nabil} \cite{AroraAnand}
    Some of the constants used in this algorithm include the maximum number of generations and the switch probability, which are generally configurable and vary 
    from problem to problem. \\
    Proposed in 1995, particle swarm optimization (PSO) can be considered the grandfather of some of the other algorithms discussed here. PSO is an algorithm in 
    which particles swarm towards an optimal goal. A particle has a few components:
    \begin{itemize}
        \item values for each variable
        \item a velocity for each variable
        \item the personal best solution that has been found
        \item the global best solution that has been found by the swarm
    \end{itemize}
    The PSO algorithm can be expressed by the following pseudocode:
    \begin{lstlisting}
        define ParticleSwarmOptimization():
            particles := initial random population of size n, each with random velocities
            global_best := particles
            while stopping condition not met do:
                for particle in particles do:
                    if particle->position > particle->best then:
                        particle->best := particle->position
                    end if 
                    for best in global_best do:
                        if particle->best > best then:
                            global_best := global_best + particle->best
                            global_best := global_best - best
                        end if
                    particle->position := particle->position + particle->velocity
                    particle->update_velocity()
                end for
            end while
            return global_best
    \end{lstlisting}
    Note that \(particle \rightarrow update\_velocity()\) is left as a function in this pseudocode. The calculation for the updated velocity has three components:
    \begin{itemize}
        \item intertia, or \(w \times particle \rightarrow velocity\)
        \item the cognitive component, which is a \(C \times random_1 \times (particle \rightarrow best - particle \rightarrow value)\)
        \item the social component, which is a \(S \times random_2 \times (global\_best - particle \rightarrow value)\)
    \end{itemize}
    where \(w\), \(C\), and \(S\) are all predefined constants and the random variables have floating point values between 0 and 1. It then returns the sum of 
    the components. \cite{EberhartKennedy} \cite{KhanesarTavakoliTeshnehlabShoorehdeli} \\
    SPEA-2 (as well as NSGA-II), while still using genetic components, are not named for a natural metaphor. SPEA-2 proceeds as follows:
    \begin{lstlisting}
        define SPEA-2():
            population := initial random population of size n
            archive := empty set
            generation := 0
            while generation < MAX_GENERATIONS do:
                dominated := empty set
                for individual in population + archive do:
                    for other in population + archive do:
                        if other dominates individual then:
                            dominated := dominated + individual
                        else if individual dominates other then:
                            dominated := dominated + other
                        end if
                    end for
                end for
                non_dominated := (population + archive) - dominated
                archive := non_dominated
                if |archive| < n then:
                    dominated := sort_by_domination(dominated)
                    archive := archive + dominated
                end if
                if |archive| > n then:
                    archive := truncate(archive)
                end if
                population := binary_tournament_selection()
            return archive
    \end{lstlisting}
    Several functions have been left as functions in this pseudocode: \(dominates\), \(sort\_by\_domination\), \(truncate\), and \(binary\_tournament\_selection\). 
    \(dominates\) is a common function in multi-objective solvers; it is used to compare solutions without having to consolidate the objective functions
    into a single, comparable value. Each objective value is compared to the respective objective value of another solution. If all are equal or greater and 
    at least one is strictly greater, then the solution dominates the other solution. Naturally, this has several useful properties such as the transitive 
    property. Sorting by domination, as in \(sort\_by\_domination\), is to put all solutions in an order such that no solution ahead of a given solution dominates 
    that solution. The \(truncate\) operation simply shortens the list to the proper \(n\) size required for the next iteration. Since the list is sorted (at 
    least at the rear of the list), this means that the solutions that are discarded are the most dominated and will not reveal the best results. Finally, 
    \(binary\_tournament\_selection\) uses a binary tournament selection method to select the next generation of solutions. First, the prospective individuals
    are sorted by domination. Then, according to their rank in the order, they are assigned probabilities for selection. The non-dominated have the best 
    probability of being chosen, while the most dominated have the worst probability of being chosen. Then, the next population is randomly chosen from this 
    pool of individuals. \cite{ZitzlerLaumannsThiele} \cite{KaucicMoradiMirzazadeh} \\
    In SPEA-2, sorting is heavily relied upon. Non-dominated Sorting Genetic Algorithm II relies even more heavily on sorting a population and picking the best from 
    the front of the list. There are two functions that NSGA-II uses: \(fast\_non\_dominated\_sort\) and \(crowding\_distance\_assignment\). \(fast\_non\_dominated\_sort\)
    is as follows:
    \begin{lstlisting}
        define fast_non_dominated_sort(population):
            front = set with a single empty set as the first item
            for individual_p in population do:
                individual_p->dominates := empty set
                individual_p->is_dominated_count := 0
                for individual_q in population do:
                    if individual_p dominates individual_q then:
                        individual_p->dominates := individual_p->dominates + individual_q
                    else if individual_q dominates individual_p then:
                        individual_p->is_dominated_count = individual_p->is_dominated_count + 1
                    end if
                end for
                if individual_p->is_dominated_count = 0 then:
                    individual_p->rank := 0
                    front[start] := front[start] + individual_p
                end if
            end for
            i := 0
            while front[i] is not empty do:
                next_front := empty set
                for individual_p in front[i] do:
                    for each individual_q in individual_p->dominates do:
                        individual_q->is_dominated_count := individual_q->is_dominated_count - 1
                        if individual_q->is_dominated_count = 0 then:
                            individual_q->rank := i
                            next_front := next_front + individual_q
                        end if
                    end for
                end for
                i := i + 1
                front[i] = next_front
            end while
    \end{lstlisting}
    \(crowding\_distance\_assignment\) is as follows:
    \begin{lstlisting}
        define crowding_distance_assignment(front):
            for individual in front do:
                individual->crowding_distance := 0
            end for
            for objective in individual->objectives do:
                front := sort_by_objective_value(front, objective)
                front[start]->crowding_distance, front[end]->crowding_distance := infinity
                for start + 1 < i < end - 1 do:
                    front[i]->crowding_distance = calculate_crowding_distance(front[i]->crowding_distance, front[i + 1]->objectives[objective], front[i - 1]->objectives[objective])
                end for
            end for
    \end{lstlisting}
    where \(calculate\_crowding\_distance(a, b, c)\) is the following formula:
    \begin{equation*}
        a = a + \dfrac{(b - c)}{f_{max} - f_{min}}
    \end{equation*}
    and \(f_{max}\), \(f_{min}\) are the maximum and minimum possible values for the objective in question respectively. 
    Finally, the full NSGA-II is as follows:
    \begin{lstlisting}
        define NSGA-II():
            generation := 0
            P := initial random population of size n
            while generation < MAX_GENERATIONS do:
                F = fast_non_dominated_sort(P + Q)
                next_population := empty set
                i := 0
                while |next_population| + |F[i]| < n do:
                    crowding_distance_assignment(F[i])
                    next_population := next_population + F[i]
                    i := i + 1
                end while
                next_population := next_population + F[i][begin - (n-|next_population|)]
                Q := binary_tournament_selection()
                generation := generation + 1
            end while
            return F[0]
    \end{lstlisting}
    This algorithm uses a similar \(binary\_tournament\_selection\) as SPEA-2.
    \cite{DebPratapAgarwalMeyarivan} \cite{PonsichJaimesCoello}
    \section{Implementation}
    \subsection{Code}
    The program was implemented using Python 3.8.5, which at the time of writing is the current latest stable version of Python. The libraries used during 
    implementation are as follows (along with links to documentation):
    \begin{itemize}
        \item copy (\url{https://docs.python.org/3/library/copy.html})
        \item math (\url{https://docs.python.org/3/library/math.html})
        \item datetime (\url{https://docs.python.org/3/library/datetime.html})
        \item os (\url{https://docs.python.org/3/library/os.html})
        \item gc (\url{https://docs.python.org/3/library/gc.html})
        \item unittest (\url{https://docs.python.org/3/library/unittest.html})
        \item random (\url{https://docs.python.org/3/library/random.html})
        \item abc (\url{https://docs.python.org/3/library/abc.html})
        \item time (\url{https://docs.python.org/3/library/time.html})
        \item finnhub (\url{https://finnhub.io})
    \end{itemize}
    Two of the driving philosophies behind how it was implemented were reusability and readability. An object-oriented design revolving around classes and 
    custom, generalized objects was used to acheive this. Following this pattern, there is one general problem design for all algorithms to use, one 
    problem initializer for all of the genetic algorithms that require a set of random solutions as input, and several utility classes that each algorithm
    optionally depends on. \cite{ReillyRalstonHemmendinger} The entire suite of solutions is fully unit tested as test driven development was the process of 
    implementation \cite{Beck}. The code itself is hosted at \url{https://github.com/rileytoddherman/cs890br-proj} and hopefully can become a package available 
    for use through the pip package installer.
    \subsection{Data}
    The data was gathered using the Finnhub client. Finnhub is a RESTful client that sources exchange data from ActivFinancial, EDI, QuoteMedia, or the stock 
    exchanges themselves (for example, ASX). Their market data archive goes back about 35 years for American stock exchanges and about a year for stock exchanges
    from other countries \cite{Finnhub} \cite{Allamaraju}. This data was then processed to reflect the three most important pieces of information for this project: 
    price, risk, and reward.
    \subsection{Results}
    One of the easiest methods to compare implementations of algorithms is by how long they take. Generally, if an exact algorithm takes a reasonable amount of 
    time to complete, then that is the best choice because the algorithm guarantees the best solution. If the stock market were as small as the artificially small
    problems that are being used to test this implementation, then branch and bound absolutely wins. In a rather strange turn of events, the branch and bound 
    implementation is the fastest implemented algorithm in this example. The goes against several expectations; for one, it is the most computationally and spacially
    complex of the set. One explanation for the bizarre result is the computation time taken up by some of the functions genetic algorithms use that branch and bound
    does not. Three functions in particular are computationally rather expensive, yet not fully accounted for in theory because they are not the most inefficient 
    and so at larger scales do not have as emphatic of an effect. The first is the generation of an initial pool of solutions. Branch and bound begins with a problem 
    with no assigned variables, so spends no time in setting up a pool of potential solutions. While this may seem like a very quick operation, in implementation it is 
    nearly half the time it takes branch and bound to solve the problem! This is in part due to the second function that is remarkably slow and forms a fundamental
    part of the genetic algorithms: picking a random number or choice. Python's random library uses the Marsenne Twister algorithm to generate pseudorandom numbers. 
    However, this is demonstrably not a fast process and it is used very repetitively in the genetic algorithms during a variety of functions. Using the python module
    timeit (\url{https://docs.python.org/3/library/timeit.html}) to time the gathering of a random floating point number, the result is nearly 100 CPU cycles per number. 
    Random integers from the library are worse at nearly 2000 CPU cycles (it's worth noting that the timeit library does involve about 30 cycles for its own processes).
    Finally, branch and bound does not need to compare solutions using the idea of domination. Comparing solutions with domination requires
    each variable to be considered for both of the solutions being compared, and this process happens repeatedly and often. For example, NSGA-II using a domination sorting 
    technique. Even if the sorting takes the standard \(O(n \log{n})\), this needs to be multiplied by the domination complexity and executed for every front and for 
    every generation. The cost compounds fairly quickly. On large scales, this compounding would pale in comparison to the compounding of branch and bound's time
    complexity, but on small scales these functions have an oversized impact. The fastest genetic algorithm was PSO, followed by flower pollination, NSGA-II and SPEA-2.\\
    In terms of quality of solutions, all of the algorithms found some solutions from the front branch and bound computed. All also found some solutions that were 
    dominated by other solutions from branch and bound. In terms of number of solutions, PSO consistently found the most solutions, followed by one of NSGA-II and
    SPEA-2, and lastly flower pollination. Flower pollination had the fewest solutions that were dominated by any other solutions, but in part this was because the
    algorithm also came up with the fewest solutions overall. Similarly, PSO had the most dominated solutions but in part this was due to the algorithm having the 
    most solutions in general. NSGA-II and SPEA-2 had very similar solutions; any instance where the solutions for one were significantly better than the other 
    can be explained as statisticaly anomolies and were not a persistent pattern. Both had high quality, with very few dominated solutions, as well as many of 
    the solutions that branch and bound had found. However, they had consistently fewer than half of the solutions that branch and bound showed were possible, 
    which would inhibit the construction of a truly cohesive front. The only metric which differed was the clustering of solutions. In large part due to the crowding
    distance sorting of NSGA-II, SPEA-2 (as well as PSO and flower pollination) often had clusters or pairs of solutions that would share many characteristics. 
    NSGA-II did not have the same problem. 
    \section{Conclusion}
    As successfull as Orlando the cat has been picking stocks, modelling and understanding portfolios in order for them to be optimized is a more trustworthy 
    method. The problem of portfolio optimization is a perfect match to the existing methodology of constraint programming, specifically constraint optimization 
    problem solving. The exact method of branch and bound will result in the best set of solutions, but is inefficient in terms of time and space. This algorithm
    was implemented as a constant to compare with the genetic algorithms. Genetic algorithms are an alternative to branch and bound, while not ensuring the best 
    answer, are much more efficient. Genetic algorithms generally use the common components of mutation, crossover, and selection, but the ways in which an 
    algorithm may use these common components differ greatly. The genetic algorithms implemented were Flower Pollination, Particle Swarm Optimization, NSGA-II, 
    and SPEA-2. These were implemented in Python using test-driven development, object-oriented design, and with readability and reusability in mind using data 
    gathered from the Finnhub RESTful API. Future work includes 
    \begin{itemize}
        \item stability improvements
        \item attempting to solve problems on a larger scale
        \item including vastly more objective criteria, including quantitative and qualitative
        \item environment configuration to publish on the pip python package installer.
        \item performance improvements; including, but not limited to multi-threading, using tuples, cross-compiling into C, using cloud computing, etc.
        \item implementing other algorithms including Artificial Bee Colony Optimization, Ant Colony Optimization, and others.
    \end{itemize}
    Unfortunately for William Stanley Jevons, there are no plans for implementing sun spot tracking in this implementation. As unpredictable as
    stock markets are, one of the best ideas in establishing performant portfolios is to create portfolios using genetic algorithms and large 
    amounts of data. William Peter Hamilton, outside of ridiculing his contemporaries, gives some adivce to ``spectators'' of the stock market:
    \begin{quotation}
        ``Speculation necessarily involves a large element of chance. \ldots It is the speculator himself who too often makes it a sheer gamble. \ldots
        [C]ertainly, if the amateur is to come into Wall Street and ``speculate'' with the stupidity he so frequently exhibits, the professionals there
        can show him that his kind of speculation is not a game of chance, and they will not have to cheat to do so.''
    \end{quotation}
    \cite{Hamilton}
    \newpage
    \printbibliography
\end{document}